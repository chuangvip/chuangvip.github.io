---
---

@string{aps = {American Physical Society,}}






@ARTICLE{10354012,
  selected=true,
  bibtex_show={true},
  author={Chen, Chuang and Sun, Xiao and Tu, Zhengzheng and Wang, Meng},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={AST-GCN: Augmented Spatial Temporal Graph Convolutional Neural Network for Gait Emotion Recognition}, 
  year={2024},
  volume={34},
  number={6},
  pages={4581-4595},
  doi={10.1109/TCSVT.2023.3341728},
  abstract={
  Skeleton-based methods have recently achieved good performance in deep learning-based gait emotion recognition (DL-GER). However, the current methods have two drawbacks that limit the ability to learn discriminative emotional features from gait. First, these methods do not exclude the effect of the subject’s walking orientation on emotion classification. Second, they do not sufficiently learn the implicit connections between the joints during human walking. In this paper, an augmented spatial-temporal graph convolutional neural network (AST-GCN) is introduced to solve these two problems. The interframe shift encoding (ISE) module acquires interframe shifts of joints to make the network sensitive to changes in emotion-related joint movements regardless of the subject’s walking orientation. A multichannel implicit connection inference method learns more implicit connection relations related to emotions. Notably, we unify current skeleton-based methods into a common framework that validates the most powerful feature representation capability of our AST-GCN from a theoretical perspective. In addition, we extend the skeleton-based gait dataset using posture estimation software. Experiments demonstrate that our AST-GCN outperforms state-of-the-art methods on three datasets on two tasks.
  },
  pdf={https://ieeexplore.ieee.org/document/10354012},
  abbr={TCSVT},
  preview={AST-GCN.png}
}

@INPROCEEDINGS{10219743,
  selected=true,
  bibtex_show={true},
  author={Chen, Chuang and Sun, Xiao},
  booktitle={2023 IEEE International Conference on Multimedia and Expo (ICME)}, 
  title={STA-GCN:Spatial Temporal Adaptive Graph Convolutional Network for Gait Emotion Recognition}, 
  year={2023},
  pages={1385-1390},
  doi={10.1109/ICME55011.2023.00240},
  pdf={https://ieeexplore.ieee.org/document/10219743},
  abstract={
  Graph Convolutional Neural Networks (GCNs) recently have been widely used in Gait Emotion Recognition (GER). However, the existing GCNs-based GER methods have two drawbacks that limit the ability to learn discriminative feature. In spatial modeling, context-sensitive affective feature of joint is under-extracted due to the neglect of implicit connection. In temporal modeling, multi-scale temporal feature of joint motion is under-extracted or aggregated rigidly. In this paper, we propose a novel Spatial-Temporal Adaptive Graph Convolutional Network (STA-GCN) where two main modules are introduced, respectively. Spatial Feature Learning Module (SFLM) infers context-sensitive joint implicit connection and adaptively aggregates spatial feature mined from implicit and explicit connection. Temporal Feature Learning Module (TFLM) extracts and adaptively aggregates multi-scale temporal feature of joint motion. It is worth mentioning that we first pre-train the model using hand-crafted affective feature and counterpart gait. Experimental results demonstrate our STA-GCN outperforms state-of-the-art methods in two tasks.
  },
  abbr={ICME},
  preview={STA-GCN.png}
}



@article{shu2023mutilmodalfeatureextractionattentionbased, 
      bibtex_show={true},
      title={Multimodal Feature Extraction and Attention-based Fusion for Emotion Estimation in Videos}, 
      author={Tao Shu and Xinke Wang and Ruotong Wang and Chuang Chen and Yixin Zhang and Xiao Sun},
      journal={arXiv preprint},
      year={2023},
      pdf={https://arxiv.org/abs/2303.10421}, 
      abstract={The continuous improvement of human-computer interaction technology makes it possible to compute emotions. In this paper, we introduce our submission to the CVPR 2023 Competition on Affective Behavior Analysis in-the-wild (ABAW). Sentiment analysis in human-computer interaction should, as far as possible Start with multiple dimensions, fill in the single imperfect emotion channel, and finally determine the emotion tendency by fitting multiple results. Therefore, We exploited multimodal features extracted from video of different lengths from the competition dataset, including audio, pose and images. Well-informed emotion representations drive us to propose a Attention-based multimodal framework for emotion estimation. Our system achieves the performance of 0.361 on the validation dataset. The code is available at [this https URL].},
      abbr={arXiv},
      preview={MMA.png}
}

Graduate National Scholarship

